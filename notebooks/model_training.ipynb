{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from PIL import Image\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "project_root = os.path.abspath('..')  # Adjust based on your project structure\n",
    "train_dir = os.path.join(project_root, 'data', 'train')\n",
    "val_dir = os.path.join(project_root, 'data', 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1280 images belonging to 8 classes.\n",
      "Found 320 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(8, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "40/40 [==============================] - 29s 644ms/step - loss: 1.6835 - accuracy: 0.3977 - val_loss: 1.3774 - val_accuracy: 0.4938\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 25s 630ms/step - loss: 0.9955 - accuracy: 0.6633 - val_loss: 1.0104 - val_accuracy: 0.6375\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 26s 639ms/step - loss: 0.6827 - accuracy: 0.7953 - val_loss: 0.9567 - val_accuracy: 0.6625\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 26s 636ms/step - loss: 0.5293 - accuracy: 0.8438 - val_loss: 0.8590 - val_accuracy: 0.7156\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 26s 645ms/step - loss: 0.3868 - accuracy: 0.9062 - val_loss: 0.8605 - val_accuracy: 0.7031\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 25s 618ms/step - loss: 0.2936 - accuracy: 0.9406 - val_loss: 0.8466 - val_accuracy: 0.6969\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 25s 623ms/step - loss: 0.2445 - accuracy: 0.9500 - val_loss: 0.8563 - val_accuracy: 0.7000\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 25s 621ms/step - loss: 0.1801 - accuracy: 0.9781 - val_loss: 0.8302 - val_accuracy: 0.7437\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 25s 621ms/step - loss: 0.1292 - accuracy: 0.9883 - val_loss: 0.8241 - val_accuracy: 0.7156\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 25s 634ms/step - loss: 0.1045 - accuracy: 0.9953 - val_loss: 0.8262 - val_accuracy: 0.7281\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 25s 627ms/step - loss: 0.0822 - accuracy: 0.9969 - val_loss: 0.8182 - val_accuracy: 0.7156\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 25s 634ms/step - loss: 0.0568 - accuracy: 0.9992 - val_loss: 0.8297 - val_accuracy: 0.7375\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 26s 636ms/step - loss: 0.0454 - accuracy: 1.0000 - val_loss: 0.8373 - val_accuracy: 0.7156\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 26s 640ms/step - loss: 0.0393 - accuracy: 1.0000 - val_loss: 0.8462 - val_accuracy: 0.7344\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 25s 629ms/step - loss: 0.0322 - accuracy: 1.0000 - val_loss: 0.8441 - val_accuracy: 0.7250\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 25s 612ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.8409 - val_accuracy: 0.7406\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 25s 629ms/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 0.8419 - val_accuracy: 0.7406\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 26s 646ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.8642 - val_accuracy: 0.7094\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 26s 641ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.8667 - val_accuracy: 0.7188\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 26s 649ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.8681 - val_accuracy: 0.7250\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "models_dir = os.path.join(project_root, 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "model.save(os.path.join(models_dir, 'gesture_recognition_model.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
